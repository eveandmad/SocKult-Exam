---
title: "Meta-analysis"
author: "Matilde"
date: "4/5/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r loading data}

install.packages(pacman)
pacman::p_load(tidyverse, 
               meta,
               brms,
               bayesplot,
               tidybayes,
               ggplot2,
               ggridges,
               glue,
               stringr,
               forcats,
               readxl,
               mice #for missing values
)

#Study overview
MA_d <- read_excel("Matrix_MetaAnalysis.xlsx")
rm(Matrix_MetaAnalysis)

View(MA_d)

```

## Template

### Define hypotheses / Describe variables
There is a cumulative and consistent effect of SC across studies
Outcome: 
- Change (amount of change from the first rating)
Predictors: 
- Feedback (difference between first rating and peer feedback)
Varying effect:
= study ID

### Identify your model[s] 

### Assess model quality
* Predictive prior checks
* Divergences / Rhat / ESS
* Prior/Posterior learning (betas and sds)
* Model comparison
* Predictive posterior checks

```{r}
MA_d <- read.csv("Matrix_MetaAnalysis - Sheet2.csv") 

#impute missing values
MA_d <- mice(MA_d, m = 5, print = FALSE)


#formula
MA_f <- bf(Change | se(ChangeSD) ~ 1 + Feedback + (1 + Feedback | Paper))

#get prior
get_prior(MA_f, data = MA_d, family = gaussian())

#set priors
MA_prior <- c(
              prior(normal(0,0.5), class = Intercept), #We expect mean of change to lie between -1 and 1
              prior(normal(0,0.25), class = b), #We expect slope for change according to feedback approximately between -.5 and .5
              prior(cauchy(0,0.3), class = sd), #Tau = between study variance of change
              prior(lkj(5), class = cor) #The true correlation between feedback and change
)

#prior predictive check
MA_m0 <- brm(
  MA_f,
  data = MA_d,
  family = gaussian(),
  prior = MA_prior,
  sample_prior = "only",
  chains = 2,
  cores = 2,
  iter = 4000,
  control = list(adapt_delta = 0.99)
)

pp_check(MA_m0, nsamples = 100)

#running the model with the data
MA_m1 <- brm(
  MA_f,
  data = MA_d,
  family = gaussian(),
  prior = MA_prior,
  sample_prior = T,
  iter = 4000,
  control = list(adapt_delta = 0.99)
)

pairs(MA_m1)

#Posterior predictive check
pp_check(MA_m1, nsamples = 100)

summary(MA_m1)

#Chain checks

```

### Plotting results
Forest plot
```{r}
ranef(MA_m1)

study.draws <- spread_draws(MA_m1, r_Paper[Paper,], b_Feedback) %>% 
  mutate(b_Feedback = r_Paper + b_Feedback)

?spread_draws

```



### Report results
* Model comparison
* Estimates and hypothesis testing
* Make sure to deal cautiously with interactions (e.g. plot!)
* Make sure to let the reader know the effects of the estimates on the outcome scale (if generalized linear model)


### Define your hypotheses
People will conform to peer feedback, that is, they will change according to the feedback.

### Report

### Methods and Materials
#### Participants

#### Experimental setup
In order to investigate whether implicit social conformity can be found beyond regression to the mean, we relied on the XNXN paradigm. DESCRIPTION OF THE PARADIGM.

#### Statistical analysis
The underlying causal diagram was formalized using Directed Acyclical Graphs in Figure XX. The diagram indicates the necessity to control for BLABLA in order to avoid BLABLA.
In order to confirm the adequacy of such approach we set up a simulation of the data generating process and compared N statistical models in their ability to infer the true parameters.
The simulation involved BLABLA
The results indicated that the model including BLABLA could indeed recover the true parameter with high precision. See plot/table.
We therefore constructed two models on the actual data from the experiment, in order to assess whether Feedback had an effect on the second rating.
As outcome we chose Change (between own ratings) modeled relying on a Gaussian likelihood. In the first model we only used First Rating as a predictor, in the second we added Feedback. Both models had a full varying structure of the predictors, varying both by Participant and by Face stimulus.
As priors we chose blabla and checked with prior predictive checks whether the scale of the expected outcomes was correct (e.g. avoiding impossible change values, such as + 100). See figure S1.
We tested for model quality in terms of no divergences, Rhat < 1.05, effective samples above 200, posterior predictive checks, visual inspection of the markov chains (raw and ranked) and assessment as to whether the posterior had learned from the data (contrast with prior). Model quality was ensured, details are reported in the appendix.
We compared the two models using loo based stacking weights model comparison (REF).
If the model including feedback was reliably better than the model without feedback, we would then more specifically test our hypothesis of implicit conformity relying on evidence ratio, that is, the amount of posterior samples compatible with our hypothesis divided the amount of incompatible posterior samples.
The modeling was performed relying on R 4.0.0, RStudio blalb, tidyverse, brms, Stan (REFS)

### Results

Model comparison revealed that no model was clearly better than the other, suggesting caution when testing hypotheses on our theoretically motivated model, which includes Feedback.
As expected we saw a credible positive effect of feedback on the change in rating beyond regression to the mean (B = 0.02, SE= 0.01, 95% CIs blabla, ER = 27.). See figure blabla. 
The effect was small, but quite consistent across participants, see figure blabla.
For full details on the model see Table S1.
