---
title: "SC script"
author: "Matilde"
date: "5/8/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r loading data}

pacman::p_load(tidyverse, 
               brms,
               bayesplot,
               viridis,
               stringr
)


#Loading data from google forms
part1 <- read_csv("Exam project spring 2020_part 1.csv")

part2 <- read_csv("Exam project spring 2020_part 2.csv")

#Loading data from oTree
sc_round1 <- read_csv("sc_round1 (accessed 2020-05-08).csv")
sc_round2 <- read_csv("sc_round2 (accessed 2020-05-08).csv")



```

## Template

### Data pre-processing
Cleaning data and getting it into the right format

```{r}

##Cleaning the google forms##
##Part1
p1 <- part1 %>% 
  select("ID" = "Unique ID of own choice (important to remember for part 2)",
         "URL" = "Unique URL code:")

#Getting URL code from web-link
p1$URL <- str_remove(p1$URL, "https://socialcognition.au.dk/p/") 
p1$URL <- str_remove_all(p1$URL, '[:punct:].*') 

#renaming chjac777
p1[2,1] <- "chjac777"
p1[2,2] <- "c1bytu87"
#remaning Christian
p1[3,2] <- "p16vvjzi"
#renaming PCWU
p1[5,2] <- "snkuwas9"

##Part 2
p2 <- part2 %>% 
  select("ID" = "Self-chosen ID (the same as the one used for the first part)",
         "URL" = "Unique ID from URL:")
#Getting URL code from web-link
p2$URL <- str_remove(p2$URL, "https://socialcognition.au.dk/p/") 
p2$URL <- str_remove_all(p2$URL, '[:punct:].*') 

##Cleaning oTree data##
##Round 1
sc1 <- sc_round1 %>% 
  select("ParticipantN" = "participant.id_in_session",
         "URL" = "participant.code",
         "Round" = "participant._current_app_name",
         "FirstRating" = "player.rating1",
         "GroupRating" = "player.TPrating",
         "FaceID" = "player.faceid")
sc1 <- na.omit(sc1)

##Round 2

sc2 <- sc_round2 %>% 
  select("ParticipantN" = "participant.id_in_session",
         "URL" = "participant.code",
         "Round" = "participant._current_app_name",
         "SecondRating" = "player.rating2",
         "FaceID" = "player.faceid")
sc2 <- na.omit(sc2)

#Binding columns
d1 <- merge(p1, sc1, by = "URL")
d2 <- merge(p2, sc2, by = "URL")
d <- merge(d1, d2, by = c("FaceID","ID"))

#Creating feedback
d$Feedback <- d$GroupRating - d$FirstRating

#Creating raw change scores
d$RawChange <- d$SecondRating - d$FirstRating

```


### Define your hypotheses
People will conform to peer feedback, that is, they will change according to the feedback (effect > 0.17)

### Describe variables
Outcome: 
- Change (amount of change from the first rating)
Predictors: 
- Feedback (difference between first rating and peer feedback)
- FirstRating (starting point, e.g. if early rating is 8, difficult to go up!)
Varying effects:
- ID: participant ID
- FaceID: stimulus ID

### Identify your model[s] 
* likelihood function: Change is numeric and goes from -6 to +6. Roughly gaussian?
* formula: 
* priors

```{r}

SocConformity_f1 <- bf(Change ~ 1 + FirstRating + Feedback + 
                         (1 + FirstRating + Feedback | ID) + 
                         (1 + FirstRating + Feedback | FaceID))

get_prior(SocConformity_f1, d, family = gaussian)

SocConformity_prior <- c(
  prior(normal(0, 1), class = Intercept),
  prior(normal(0, .3), class = b),
  prior(normal(0, .3), class = sd),
  prior(lkj(5), class = cor),
  prior(normal(0,1), class = sigma)
)

SocConformity_m_prior <- brm(
  SocConformity_f1,
  d,
  family = gaussian,
  prior = SocConformity_prior,
  sample_prior = "only",
  chains = 2,
  cores = 2,
  control = list(
    adapt_delta = 0.99,
    max_treedepth = 20
  )
)

pp_check(SocConformity_m_prior, nsamples=100)

```

## Fit the model and assess model quality


```{r}
SocConformity_m <- brm(
  SocConformity_f1,
  d,
  family = gaussian,
  prior = SocConformity_prior,
  sample_prior = T,
  chains = 2,
  cores = 2,
  iter = 4000,
  control = list(
    adapt_delta = 0.99,
    max_treedepth = 20
  )
)

summary(SocConformity_m)

pp_check(SocConformity_m, nsamples=100)


color_scheme_set("viridis")
mcmc_trace(SocConformity_m,
           pars = c("b_Intercept", "b_Feedback", 
           "sd_FaceID__Intercept", "sd_FaceID__Feedback",
           "sd_ID__Intercept", "sd_ID__Feedback")) + 
  theme_classic()
mcmc_rank_overlay(SocConformity_m,
                  pars = c("b_Intercept", "b_Feedback", 
           "sd_FaceID__Intercept", "sd_FaceID__Feedback",
           "sd_ID__Intercept", "sd_ID__Feedback")) + theme_classic()

# The posteriors have moved or gotten more confident than the priors
plot(hypothesis(SocConformity_m,"Intercept > 0"))
plot(hypothesis(SocConformity_m,"Intercept > 0", class="sd", group="FaceID"))
plot(hypothesis(SocConformity_m,"Intercept > 0", class="sd", group="ID"))
plot(hypothesis(SocConformity_m,"Feedback > 0"))
plot(hypothesis(SocConformity_m,"Feedback > 0", class="sd", group="FaceID"))
plot(hypothesis(SocConformity_m,"Feedback > 0", class="sd", group="ID"))
plot(conditional_effects(SocConformity_m), points=T, rug=T)[[1]] + theme_classic()

```

## Model comparison

```{r}
# What would make sense?
## Model without rating?
## Model without feedback?


SocConformity_f0 <- 
  bf(Change ~ 1 + FirstRating + 
       (1 + FirstRating | ID) + 
       (1 + FirstRating | FaceID))

SocConformity_m0 <- update(SocConformity_m, formula = SocConformity_f0, newdata = d)

SocConformity_m0 <- add_criterion(SocConformity_m0,criterion="loo")
SocConformity_m <- add_criterion(SocConformity_m,criterion="loo")
loo_compare(SocConformity_m0,SocConformity_m)
loo_model_weights(SocConformity_m0,SocConformity_m)
```

## Results and hyp testing
```{r}
summary(SocConformity_m)
hypothesis(SocConformity_m,"Feedback > 0")

## Visualize 
conditional_effects(SocConformity_m)

xx <- predict(SocConformity_m, summary=T)
d <- cbind(d,xx)
d$ID <- as.factor(d$ID)
ggplot(d) + 
  geom_point(aes(Feedback,Change, color = ID, group=ID)) + 
  geom_smooth(method=lm, se=F, aes(Feedback,Change, color = ID))

X <- hypothesis(SocConformity_m, "Feedback > 0",group = "ID", scope = "coef")
X$hypothesis %>%
  left_join(distinct(d, Group = ID)) %>% 
  mutate(id = factor(Group), Conformity = Estimate) %>%
  ggplot(aes(Conformity, id)) +
  geom_errorbarh(aes(xmin = CI.Lower, xmax = CI.Upper)) +
  geom_point() + theme_classic()

```

### Report

### Methods and Materials
#### Participants

#### Experimental setup
In order to investigate whether implicit social conformity can be found beyond regression to the mean, we relied on the XNXN paradigm. DESCRIPTION OF THE PARADIGM.

#### Statistical analysis
The underlying causal diagram was formalized using Directed Acyclical Graphs in Figure XX. The diagram indicates the necessity to control for BLABLA in order to avoid BLABLA.
In order to confirm the adequacy of such approach we set up a simulation of the data generating process and compared N statistical models in their ability to infer the true parameters.
The simulation involved BLABLA
The results indicated that the model including BLABLA could indeed recover the true parameter with high precision. See plot/table.
We therefore constructed two models on the actual data from the experiment, in order to assess whether Feedback had an effect on the second rating.
As outcome we chose Change (between own ratings) modeled relying on a Gaussian likelihood. In the first model we only used First Rating as a predictor, in the second we added Feedback. Both models had a full varying structure of the predictors, varying both by Participant and by Face stimulus.
As priors we chose blabla and checked with prior predictive checks whether the scale of the expected outcomes was correct (e.g. avoiding impossible change values, such as + 100). See figure S1.
We tested for model quality in terms of no divergences, Rhat < 1.05, effective samples above 200, posterior predictive checks, visual inspection of the markov chains (raw and ranked) and assessment as to whether the posterior had learned from the data (contrast with prior). Model quality was ensured, details are reported in the appendix.
We compared the two models using loo based stacking weights model comparison (REF).
If the model including feedback was reliably better than the model without feedback, we would then more specifically test our hypothesis of implicit conformity relying on evidence ratio, that is, the amount of posterior samples compatible with our hypothesis divided the amount of incompatible posterior samples.
The modeling was performed relying on R 4.0.0, RStudio blalb, tidyverse, brms, Stan (REFS)

### Results

Model comparison revealed that no model was clearly better than the other, suggesting caution when testing hypotheses on our theoretically motivated model, which includes Feedback.
As expected we saw a credible positive effect of feedback on the change in rating beyond regression to the mean (B = 0.02, SE= 0.01, 95% CIs blabla, ER = 27.). See figure blabla. 
The effect was small, but quite consistent across participants, see figure blabla.
For full details on the model see Table S1.
