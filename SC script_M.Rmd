---
title: "SC script"
author: "Matilde"
date: "5/8/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r loading data}

pacman::p_load(tidyverse, 
               brms,
               bayesplot,
               viridis,
               stringr,
               lme4,
               lmerTest, 
               rstatix
)


#Loading data from google forms
demo <- read_csv("Exam project spring 2020_part 1 (Responses).csv") %>% 
  mutate()

part2 <- read_csv("Exam project spring 2020_part 2 (Responses).csv")

p43 <- read_csv("Completed participant IDs - 44.csv") %>% 
  select(-"Finished?")

p41 <- read_csv("Completed participant IDs - 41.csv") 

p38 <- read_csv("Completed participant IDs - clean.csv")

#Loading data from oTree
sc_round1 <- read_csv("sc_round1 (accessed 2020-05-14).csv") 
  
sc_round2 <- read_csv("sc_round2 (accessed 2020-05-14).csv")



```

## Template

### Data pre-processing
Cleaning data and getting it into the right format

```{r}

##Cleaning the google forms##
summary(part1)
##Part1
p1 <- part1 %>% 
  select("ID" = "Unique ID of own choice (important to remember for part 2)",
         "URL" = "Step 3: Paste the unique URL link below")

#Getting URL code from web-link
p1$URL <- str_remove(p1$URL, "https://socialcognition.au.dk/p/") 
p1$URL <- str_remove_all(p1$URL, '[:punct:].*') 

# #renaming chjac777
# p1[2,1] <- "chjac777"
# p1[2,2] <- "c1bytu87"
# #remaning Christian
# p1[3,2] <- "p16vvjzi"
# #renaming PCWU
# p1[5,2] <- "snkuwas9"

##Part 2##
summary(part2)
p2 <- part2 %>% 
  select("ID" = "Self-chosen ID (the same as the one used for the first part)",
         "URL" = "Step 3: Paste the unique URL link below")
#Getting URL code from web-link
p2$URL <- str_remove(p2$URL, "https://socialcognition.au.dk/p/") 
p2$URL <- str_remove_all(p2$URL, '[:punct:].*') 

#renaming
p38$`URL round 2` <- gsub("8gnlaox0", "8gnloax0", p38$`URL round 2`)

##Cleaning oTree data##
##Round 1
sc1 <- sc_round1 %>% 
  select("ParticipantN" = "participant.id_in_session",
         "URL round 1" = "participant.code",
         "Round" = "participant._current_app_name",
         "FirstRating" = "player.rating1",
         "GroupRating" = "player.TPrating",
         "FaceID" = "player.faceid")
sc1 <- na.omit(sc1)

##Round 2

sc2 <- sc_round2 %>% 
  select("ParticipantN" = "participant.id_in_session",
         "URL round 2" = "participant.code",
         "Round" = "participant._current_app_name",
         "SecondRating" = "player.rating2",
         "FaceID" = "player.faceid") 
sc2 <- na.omit(sc2)

#Binding columns
d1 <- merge(p1, sc1, by = "URL")
d2 <- merge(p2, sc2, by = "URL")
d <- merge(d1, d2, by = c("FaceID","ID"))

#For cleaned data
d381 <- merge(p38, sc1, by = "URL round 1")
d382 <- merge(p38, sc2, by = "URL round 2")
d38 <- merge(d381, d382, by = c("FaceID","ID"))

d38ID <- as.data.frame(unique(d38$ID))
count(d38ID)

#For cleaned data with missing ratings
d411 <- merge(p41, sc1, by = "URL round 1")
d412 <- merge(p41, sc2, by = "URL round 2")
d41 <- merge(d411, d412, by = c("FaceID","ID"))

d41ID <- as.data.frame(unique(d41$ID))
count(d41ID)

#Creating feedback
d$Feedback <- d$GroupRating - d$FirstRating
d <- d %>%
  mutate(
    Feedback = ifelse(GroupRating==0, NA, Feedback)
  )

#Creating feedback for 38
d38$Feedback <- d38$GroupRating - d38$FirstRating
d38 <- d38 %>%
  mutate(
    Feedback = ifelse(GroupRating==0, NA, Feedback)
  )

#Feedback 2 
d38$Feedback2 <- d38 %>% 
  Feedback2 = ifelse(Feedback %in% c(-2, -3), -2.5, Feedback)

#Creating raw change scores
d38$Change <- d38$SecondRating - d38$FirstRating

##Checking for regression to the mean
d38_2 <- subset(d38, is.na(Feedback))
d38_2ID <- as.data.frame(unique(d38_2$ID))
count(d38_2ID)

#What is the tendency to shrink values when there is no feedback 
m <- lmer(SecondRating ~ 1 + FirstRating + (1  | ID), 
          subset(d38, is.na(Feedback)), REML=F)
summary(m)
#Estimate of a correlation between first and second rating. 1 = perfectly correlated, the smaller it is the more regression to the mean
ggplot(subset(d38, is.na(Feedback)), aes(FirstRating,SecondRating)) + 
  geom_point() +
  geom_smooth(method=lm)

d38$Changeabs <- abs(d38$Change)
mean(d38$Changeabs)

```


### Define your hypotheses
People will conform to peer feedback, that is, they will change according to the feedback (effect > 0.17)

### Describe variables
Outcome: 
- Change (amount of change from the first rating)
Predictors: 
- Feedback (difference between first rating and peer feedback)
- FirstRating (starting point, e.g. if early rating is 8, difficult to go up!)
Varying effects:
- ID: participant ID
- FaceID: stimulus ID

### Identify your model[s] 
* likelihood function: Change is numeric and goes from -6 to +6. Roughly gaussian?
* formula: Change ~ 1 + FirstRating + Feedback + 
                         (1 + FirstRating + Feedback | ID) + 
                         (1 + FirstRating + Feedback | FaceID))
* priors

```{r}

SocConformity_f1 <- bf(Change ~ 1 + FirstRating + Feedback + 
                         (1 + FirstRating + Feedback | ID) + 
                         (1 + FirstRating + Feedback | FaceID))

get_prior(SocConformity_f1, d38, family = gaussian)

SocConformity_prior <- c(
  prior(normal(0, 1), class = Intercept),
  prior(normal(0, .3), class = b),
  prior(normal(0, .3), class = sd),
  prior(lkj(5), class = cor),
  prior(normal(0,1), class = sigma)
)

SocConformity_m_prior <- brm(
  SocConformity_f1,
  d38,
  family = gaussian,
  prior = SocConformity_prior,
  sample_prior = "only",
  chains = 2,
  cores = 2,
  control = list(
    adapt_delta = 0.99,
    max_treedepth = 20
  )
)

pp_check(SocConformity_m_prior, nsamples=100)

```

## Fit the model and assess model quality


```{r}
SocConformity_m <- brm(
  SocConformity_f1,
  d38,
  family = gaussian,
  prior = SocConformity_prior,
  sample_prior = T,
  chains = 4,
  cores = 2,
  iter = 4000,
  control = list(
    adapt_delta = 0.99,
    max_treedepth = 20
  )
)

summary(SocConformity_m)

pp_check(SocConformity_m, nsamples=100)

d38low <- subset(d38, Feedback %in% c(-3,-2))
d38same <- subset(d38, Feedback %in% c(-1, 0, 1))
d38high <- subset(d38, Feedback %in% c(3,2))

d38low %>% 
  get_summary_stats(Change,type = "mean_sd")
d38same %>% 
  get_summary_stats(Change,type = "mean_sd")
d38high %>% 
  get_summary_stats(Change,type = "mean_sd")


#Assessing chains
#Traceplot
color_scheme_set("viridis")
mcmc_trace(SocConformity_m,
           pars = c("b_Intercept", "b_Feedback", 
           "sd_FaceID__Intercept", "sd_FaceID__Feedback",
           "sd_ID__Intercept", "sd_ID__Feedback")) + 
  theme_classic()
#Overlay
mcmc_rank_overlay(SocConformity_m,
                  pars = c("b_Intercept", "b_Feedback", 
           "sd_FaceID__Intercept", "sd_FaceID__Feedback",
           "sd_ID__Intercept", "sd_ID__Feedback")) + theme_classic()

# The posteriors have moved or gotten more confident than the priors
plot(hypothesis(SocConformity_m,"Intercept > 0"))
plot(hypothesis(SocConformity_m,"Intercept > 0", class="sd", group="FaceID"))
plot(hypothesis(SocConformity_m,"Intercept > 0", class="sd", group="ID"))
plot(hypothesis(SocConformity_m,"Feedback > 0"))
plot(hypothesis(SocConformity_m,"Feedback > 0", class="sd", group="FaceID"))
plot(hypothesis(SocConformity_m,"Feedback > 0", class="sd", group="ID"))
plot(conditional_effects(SocConformity_m), points=T, rug=T)[[1]] + theme_classic()

```

Models indexed by group opinion
```{r}
SocConformity_low <- update(SocConformity_m, formula = SocConformity_f1, newdata = d38low)
SocConformity_same <- update(SocConformity_m, formula = SocConformity_f1, newdata = d38same)
SocConformity_high <- update(SocConformity_m, formula = SocConformity_f1, newdata = d38high)

summary(SocConformity_low)
summary(SocConformity_same)
summary(SocConformity_high)
```

Models indexed by group opinion: SAME
```{r}

SocConformity_same <- bf(Change ~ 1 + FirstRating + Feedback + 
                         (1 + FirstRating + Feedback | ID) + 
                         (1 + FirstRating + Feedback | FaceID))

get_prior(SocConformity_same, d38same, family = gaussian)

SocConformity_prior_low <- c(
  prior(normal(0, 0.2), class = Intercept),
  prior(normal(0, -.3), class = b),
  prior(normal(0, .3), class = sd),
  prior(lkj(5), class = cor),
  prior(normal(0,1), class = sigma)
)

SocConformity_low_prior <- brm(
  SocConformity_low,
  d38low,
  family = gaussian,
  prior = SocConformity_prior_low,
  sample_prior = "only",
  chains = 2,
  cores = 2,
  control = list(
    adapt_delta = 0.99,
    max_treedepth = 20
  )
)

pp_check(SocConformity_m_prior, nsamples=100)

SocConformity_low_m <- brm(
  SocConformity_low,
  d38low,
  family = gaussian,
  prior = SocConformity_prior_low,
  sample_prior = T,
  chains = 2,
  cores = 2,
  control = list(
    adapt_delta = 0.99,
    max_treedepth = 20
  )
)


## Model comparison

```{r}
# What would make sense?
## Model without rating?
## Model without feedback?


SocConformity_f0 <- 
  bf(Change ~ 1 + FirstRating + 
       (1 + FirstRating | ID) + 
       (1 + FirstRating | FaceID))

SocConformity_m0 <- update(SocConformity_m, formula = SocConformity_f0, newdata = d)

SocConformity_m0 <- add_criterion(SocConformity_m0,criterion="loo")
SocConformity_m <- add_criterion(SocConformity_m,criterion="loo")
loo_compare(SocConformity_m0,SocConformity_m)
loo_model_weights(SocConformity_m0,SocConformity_m)
```

## Results and hyp testing
```{r}
summary(SocConformity_m)
hypothesis(SocConformity_m,"Feedback > 0")

## Visualize 
conditional_effects(SocConformity_m)

xx <- predict(SocConformity_m, summary=T)
d <- cbind(d,xx)
d$ID <- as.factor(d$ID)
ggplot(d) + 
  geom_point(aes(Feedback,Change, color = ID, group=ID)) + 
  geom_smooth(method=lm, se=F, aes(Feedback,Change, color = ID))

X <- hypothesis(SocConformity_m, "Feedback > 0.17",group = "ID", scope = "coef")
X$hypothesis %>%
  left_join(distinct(d, Group = ID)) %>% 
  mutate(id = factor(Group), Conformity = Estimate) %>%
  ggplot(aes(Conformity, id)) +
  geom_errorbarh(aes(xmin = CI.Lower, xmax = CI.Upper)) +
  geom_point() + theme_classic()

```

### Report

### Methods and Materials
#### Participants

#### Experimental setup
In order to investigate whether implicit social conformity can be found beyond regression to the mean, we relied on the XNXN paradigm. DESCRIPTION OF THE PARADIGM.

#### Statistical analysis
The underlying causal diagram was formalized using Directed Acyclical Graphs in Figure XX. The diagram indicates the necessity to control for BLABLA in order to avoid BLABLA.
In order to confirm the adequacy of such approach we set up a simulation of the data generating process and compared N statistical models in their ability to infer the true parameters.
The simulation involved BLABLA
The results indicated that the model including BLABLA could indeed recover the true parameter with high precision. See plot/table.
We therefore constructed two models on the actual data from the experiment, in order to assess whether Feedback had an effect on the second rating.
As outcome we chose Change (between own ratings) modeled relying on a Gaussian likelihood. In the first model we only used First Rating as a predictor, in the second we added Feedback. Both models had a full varying structure of the predictors, varying both by Participant and by Face stimulus.
As priors we chose blabla and checked with prior predictive checks whether the scale of the expected outcomes was correct (e.g. avoiding impossible change values, such as + 100). See figure S1.
We tested for model quality in terms of no divergences, Rhat < 1.05, effective samples above 200, posterior predictive checks, visual inspection of the markov chains (raw and ranked) and assessment as to whether the posterior had learned from the data (contrast with prior). Model quality was ensured, details are reported in the appendix.
We compared the two models using loo based stacking weights model comparison (REF).
If the model including feedback was reliably better than the model without feedback, we would then more specifically test our hypothesis of implicit conformity relying on evidence ratio, that is, the amount of posterior samples compatible with our hypothesis divided the amount of incompatible posterior samples.
The modeling was performed relying on R 4.0.0, RStudio blalb, tidyverse, brms, Stan (REFS)

### Results

Model comparison revealed that no model was clearly better than the other, suggesting caution when testing hypotheses on our theoretically motivated model, which includes Feedback.
As expected we saw a credible positive effect of feedback on the change in rating beyond regression to the mean (B = 0.02, SE= 0.01, 95% CIs blabla, ER = 27.). See figure blabla. 
The effect was small, but quite consistent across participants, see figure blabla.
For full details on the model see Table S1.
